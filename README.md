

## Summary
- Small model used throughout examples: [granite-3.3-2b-instruct](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct)

- [vLLM](https://docs.vllm.ai/en/stable/index.html) is used as the runtime for LLM inference and serving.


## Localhost serving of a model using `podman` or `docker` with CPU only.

Instructions: [docker-podman-cpu-only-localhost.md](docker-podman-cpu-only-localhost.md)

 - You can use either `podman` or `docker`.


##  [Redistributable Kserve Modelcar Container Images](https://kserve.github.io/website/latest/modelserving/storage/oci/)

Instructions: [kserve/README.md](kserve/README.md)